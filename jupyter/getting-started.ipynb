{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To deal with alerts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, please execute this cell.\n",
    "That will create test-alert to alertmanager."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "url='http://127.0.0.1:9093/api/v1/alerts'\n",
    "headers={\"Content-Type\": \"application/json\"}\n",
    "data=[{\"labels\":{\"alertname\":\"execjson-test-alert\", \"service\": \"files\"}}]\n",
    "requests.post(url, data=json.dumps(data), headers=headers)\n",
    "data=[{\"labels\":{\"alertname\":\"test-alert\"}}]\n",
    "requests.post(url, data=json.dumps(data), headers=headers)\n",
    "data=[{\"labels\":{\"alertname\":\"jupyter-upload-test\"}}]\n",
    "requests.post(url, data=json.dumps(data), headers=headers)\n",
    "\n",
    "import os\n",
    "myip=os.popen(\"ip addr show | grep -w inet | grep -v 127.0.0.1 | awk {'print $2'} | awk -F/ '{print $1}'\").read().rstrip()\n",
    "#print(myip)\n",
    "with open('getting-started.ipynb') as f:\n",
    "    ipynb_str=f.read().replace('ip:', myip+':')\n",
    "with open('readme.ipynb', 'w') as f:\n",
    "    f.write(ipynb_str)\n",
    "print('please click this link: http://{0}:8888/notebooks/readme.ipynb'.format(myip))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then  \n",
    "http://ip:9093 (alertmanager),  \n",
    "will see several alerts,  \n",
    "and webhook will be sent to  \n",
    "http://ip:80 (execjson),  \n",
    "and  \n",
    "http://ip:5001 (open-pager-url).\n",
    "\n",
    "Check that execjson had done a job based on the alert, with execjson joblog feature.  \n",
    "When open-pager-url is accessed, you'll see two windows are opened.  \n",
    "One is for google, which contain some information for an alert :(, the other is for jupyter notebook, which contains all the information to deal that alert.\n",
    "\n",
    "\n",
    "After that, check kibana, to see json resulted from execjson.  \n",
    "http://ip:5601 (kibana)\n",
    "\n",
    "\n",
    "\n",
    "There are some other applications for some usecases.  \n",
    "http://ip:9090 (prometheus, please type node_load1, then press execute and graph)  \n",
    "http://ip:5000 (applcon, input 1 in start point and press DFS search )  \n",
    "http://ip:9200 (elasticsearch, since this is mostly for data management, please check kibana instead)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To deal with Service Request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To deal with service request, you can firstly try to open\n",
    "http://ip:80 (execjson)\n",
    "\n",
    "Your customer can define their request based on this web page, that will review their input instead of you :),  \n",
    "except for some requests such as the one which technically possible, but you have to judge from finantial point of view :(\n",
    "\n",
    "After you think their SR is ok, you can import SR to execjson and push 'exec'.\n",
    "\n",
    "Then you can check the result from joblog link, dashboard, and kibana.\n",
    "\n",
    "Customer also could see joblog link, although their access to the jobs is restricted by apache authorization.\n",
    "\n",
    "\n",
    "\n",
    "For a demo purpose, you can execute test-exception job, and you could see execjson will do some task based on the job definition. You can also download test.json and upload to execjson and exec it. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To add a new node to CMDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To add a new node to CMDB,\n",
    "you firstly should add IPAM to some db.\n",
    "\n",
    "At this time, it uses manual operation to get this, but in the future, it might use full-feature DCIM, such as netbox instead.  \n",
    "please type  \n",
    "$ kubectl get pod -o wide  \n",
    "to get IPs for containers\n",
    "\n",
    "After that, you can discover some values with the help of ansible, and applconn will import all the values to kibana, and visualize the connectivity from several point of view, which include application, server,  network, storage, container, although that has to be customized heavily for your environment :(\n",
    "\n",
    "For this demo, some containers with sshd will be set up, which implement haproxy and apache, and with your defining ip, user, password, it will start discovery and visualize them by d3.js and searched by kibana.\n",
    "\n",
    "For the detailed instruction, please open this url  \n",
    "http://ip:8888/notebooks/setup_haproxy_and_httpd.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do some job at the new node from execjson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After applconn recognize some ip, you can kick some basic jobs at that node, which include scp files from and to that node and kick some command on that node.\n",
    "\n",
    "For this demo, you can do mkdir job and execshell job on the new node, after you did setpublickey job with root passwd."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create self-healing job from execjson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since execjson's job is kicked by web-service, when prometheus found some alert, alertmanager can kick execjson to do recovery job.\n",
    "\n",
    "Since most of the recovery jobs could use similar command with SRs, execjson might define json-based job, to deal with that alert.\n",
    "\n",
    "For that purpose, you can create a job to deal with haproxy-restart and httpd-restart, and export them to some directory on execjson node, and kick them from alertmanager.\n",
    "\n",
    "Next cell contains some logic to do that task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrap up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In old days, we set up all the boxes by hand, and configure other tools, such as monitoring, CMDB, telemetry, separately.  \n",
    "And if you have got some pager or service requests, you have to do all of them manually.\n",
    "\n",
    "In these days, we can set up all the boxes automatically (except for hardware tasks :( ), and update other tool settings simultaneously.\n",
    "- although you have to write down a lot of plugin if your environment uses a lot o os/mw/hw etc\n",
    "\n",
    "You can also define a lot of jobs to deal with pager and service requests to make most of them done automatically, and make all the task you have to do be to check and to review.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
